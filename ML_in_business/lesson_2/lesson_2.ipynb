{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 2. #Профилирование пользователей. Сегментация аудитории: unsupervised learning (clustering, LDA/ARTM), supervised (multi/binary classification)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Получаем векторные представления новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "len(stopword_ru)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 s, sys: 193 ms, total: 32.2 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 1s, sys: 562 ms, total: 5min 2s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь в 3 строчки обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое common_dictionary и как он выглядит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ватутин'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все просто - это словарь наших слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 4.05 s, total: 1min 9s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучили модель. Теперь 2 вопроса:\n",
    "\n",
    "1. как выглядят наши темы\n",
    "2. как получить для документа вектор значений (вероятности принадлежности каждой теме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['форвард', 'авангард', 'томаш', 'заборский', 'прокомментировать', 'игра', 'команда', 'матч', 'чемпионат', 'кхл', 'против', 'атланта', 'nnnn', 'плохой', 'матч', 'нижний', 'новгород', 'против', 'торпедо', 'настраиваться', 'первый', 'минута', 'включиться', 'заборский', 'получиться', 'забросить', 'быстрый', 'гол', 'задать', 'хороший', 'темп', 'поединок', 'играть', 'хороший', 'сторона', 'пять', 'очко', 'выезд', 'девять', 'хороший']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.16236429),\n",
       " (10, 0.032250214),\n",
       " (14, 0.07372059),\n",
       " (18, 0.54678),\n",
       " (19, 0.16358669)]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: украина военный украинский nn восток гражданин киев\n",
      "topic_1: налог мвд собственность отель наказание приговор перевод\n",
      "topic_2: рынок рост млн банк статья решение новый\n",
      "topic_3: продукция пограничный уверять приобретать открыться сопровождать шутка\n",
      "topic_4: операция пациент пострадать произойти район заявлять лечение\n",
      "topic_5: гражданин закон законопроект ракета парламент доклад запрет\n",
      "topic_6: планета министерство космос расчёт китайский китай запустить\n",
      "topic_7: греция майкл выручка челябинский греческий комиссар демократия\n",
      "topic_8: товар оплата гражданство повышаться часовой дорого датский\n",
      "topic_9: снижение обращение мужчина офицер железный таиланд терминал\n",
      "topic_10: писать доллар рейс ступень аэропорт рейтинг виза\n",
      "topic_11: путин владимир пенсия глава писать кремль пресссекретарить\n",
      "topic_12: млрд турция учреждение объём энергия су развивающийся\n",
      "topic_13: общество производить вино этаж департамент лауреат предупредить\n",
      "topic_14: проект москва nn университет русский день участник\n",
      "topic_15: женщина тело мозг обнаружить смерть мужчина найти\n",
      "topic_16: сотрудник nn млрд рф банк взрыв министр\n",
      "topic_17: рубль цена тыс млн станция составить бюджет\n",
      "topic_18: всё научный первый очень журнал россия nn\n",
      "topic_19: сибирский заполнить педагог поймать снаряд контролироваться макс\n",
      "topic_20: россия российский сша система газ nn фонд\n",
      "topic_21: земля рак возраст жизнь ребёнок градус боль\n",
      "topic_22: nn день новый первый исследование автор ракета\n",
      "topic_23: источник население россия метод риск выяснить квартира\n",
      "topic_24: погибнуть проверка конкурс египет супруг мальчик искусство\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень неплохо - большинство тем вполне можно описать о чем они"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем функцию, которая будет нам возвращать векторное представление новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.16365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>0.163641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589605</td>\n",
       "      <td>0.049050</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395873</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_0   topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  \\\n",
       "0       6      0.0  0.000000  0.00000      0.0      0.0      0.0      0.0   \n",
       "1    4896      0.0  0.000000  0.00000      0.0      0.0      0.0      0.0   \n",
       "2    4897      0.0  0.000000  0.16365      0.0      0.0      0.0      0.0   \n",
       "3    4898      0.0  0.000000  0.00000      0.0      0.0      0.0      0.0   \n",
       "4    4899      0.0  0.395873  0.00000      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   topic_7  topic_8  ...  topic_15  topic_16  topic_17  topic_18  topic_19  \\\n",
       "0      0.0      0.0  ...       0.0  0.873195       0.0  0.000000  0.000000   \n",
       "1      0.0      0.0  ...       0.0  0.738570       0.0  0.000000  0.237211   \n",
       "2      0.0      0.0  ...       0.0  0.000000       0.0  0.545608  0.163641   \n",
       "3      0.0      0.0  ...       0.0  0.000000       0.0  0.589605  0.049050   \n",
       "4      0.0      0.0  ...       0.0  0.104702       0.0  0.000000  0.053572   \n",
       "\n",
       "   topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0  0.000000  0.000000  0.000000       0.0       0.0  \n",
       "1  0.000000  0.000000  0.000000       0.0       0.0  \n",
       "2  0.000000  0.000000  0.000000       0.0       0.0  \n",
       "3  0.225657  0.000000  0.125133       0.0       0.0  \n",
       "4  0.000000  0.039401  0.000000       0.0       0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прекрасно, мы получили вектора наших новостей! И даже умеем интерпретировать получившиеся темы.\n",
    "\n",
    "Можно двигаться далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Следующий шаг - векторные представления пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим tfidf для статей пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=15000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=15000)\n",
    "tfidf.fit(users['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321929</th>\n",
       "      <td>7.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323233</th>\n",
       "      <td>7.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324181</th>\n",
       "      <td>7.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321850</th>\n",
       "      <td>7.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323631</th>\n",
       "      <td>7.154108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513087</th>\n",
       "      <td>9.294175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513085</th>\n",
       "      <td>9.294175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513083</th>\n",
       "      <td>9.294175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513094</th>\n",
       "      <td>9.294175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>9.294175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14781 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf\n",
       "321929  7.096950\n",
       "323233  7.096950\n",
       "324181  7.096950\n",
       "321850  7.096950\n",
       "323631  7.154108\n",
       "...          ...\n",
       "513087  9.294175\n",
       "513085  9.294175\n",
       "513083  9.294175\n",
       "513094  9.294175\n",
       "5407    9.294175\n",
       "\n",
       "[14781 rows x 1 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = pd.DataFrame(tfidf.idf_, index=tfidf.get_feature_names(), columns=[\"idf\"]) \n",
    "idf.sort_values(by=['idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.06896462, 0.        , 0.03421551,\n",
       "       0.04968504, 0.10160048, 0.03472021, 0.        , 0.        ,\n",
       "       0.        , 0.11997338, 0.        , 0.        , 0.17824621,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.17258951, 0.20951281, 0.02029782])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict[293622]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['articles'].iloc[33]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**У меня возникла странная проблема - в tfidf не учлись статьи с id < 10, хотя они встречались в датафрейме, на котором вычислялись в tfidf. Я некоторое время птылася разобраться с проблемой, но потом просто поставил заглушку чтобы пропускать статьи с индексом менее 10, во всей выборке их встретилось всего несколько штук. Возможно вы подскажете в чем может быть проблема?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u107120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u102277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u102444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid  churn\n",
       "0  u107120      0\n",
       "1  u102277      0\n",
       "2  u102444      0"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['type', 'roc_auc', 'precision', 'recall', 'f-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mode = [['no_idf', 'mean'],\n",
    "        ['no_idf', 'median'],\n",
    "        ['no_idf', 'max'],\n",
    "        ['idf', 'mean'],\n",
    "        ['idf', 'median'],\n",
    "        ['idf', 'max']]\n",
    "\n",
    "\n",
    "\n",
    "for m in mode:\n",
    "    \n",
    "    def get_user_embedding(user_articles_list):\n",
    "        user_articles_list = eval(user_articles_list)\n",
    "        \n",
    "        if m[0] == 'idf':\n",
    "            user_vector = np.array([(doc_dict[doc_id]*idf.loc[idf.index==str(doc_id), 'idf'][0]) for doc_id in user_articles_list if doc_id >= 10])\n",
    "        else:\n",
    "            user_vector = np.array([(doc_dict[doc_id]) for doc_id in user_articles_list])\n",
    "            \n",
    "        if m[1] == 'mean':\n",
    "            user_vector = np.mean(user_vector, 0)\n",
    "        elif m[1] == 'median':\n",
    "            user_vector = np.median(user_vector, 0)\n",
    "        else:\n",
    "            user_vector = np.max(user_vector, 0)\n",
    "            \n",
    "        \n",
    "        return user_vector\n",
    "\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "    user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                        X['churn'], random_state=0)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    fscore = fscore[~np.isnan(fscore)]\n",
    "    ix = np.argmax(fscore)\n",
    "    \n",
    "    result = result.append({'type':str(m[0] + ' ' + m[1]),\n",
    "                   'roc_auc':roc_auc_score(y_test, preds),\n",
    "                   'precision':precision[ix],\n",
    "                   'recall':recall[ix],\n",
    "                   'f-score':fscore[ix]}, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_idf mean</td>\n",
       "      <td>0.919714</td>\n",
       "      <td>0.599222</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.613546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_idf median</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.669388</td>\n",
       "      <td>0.708423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_idf max</td>\n",
       "      <td>0.963296</td>\n",
       "      <td>0.702602</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.735409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>idf mean</td>\n",
       "      <td>0.980738</td>\n",
       "      <td>0.817427</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>idf median</td>\n",
       "      <td>0.979810</td>\n",
       "      <td>0.846491</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>0.816068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>idf max</td>\n",
       "      <td>0.957298</td>\n",
       "      <td>0.785388</td>\n",
       "      <td>0.702041</td>\n",
       "      <td>0.741379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type   roc_auc  precision    recall   f-score\n",
       "0    no_idf mean  0.919714   0.599222  0.628571  0.613546\n",
       "1  no_idf median  0.956897   0.752294  0.669388  0.708423\n",
       "2     no_idf max  0.963296   0.702602  0.771429  0.735409\n",
       "3       idf mean  0.980738   0.817427  0.804082  0.810700\n",
       "4     idf median  0.979810   0.846491  0.787755  0.816068\n",
       "5        idf max  0.957298   0.785388  0.702041  0.741379"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты:\n",
    "    - применил idf в качестве весов для статей, умножая вектор тем статьи на idf. Таким образом статьи, которые читались чаще будут иметь значения вектора меньше, чем те статьи, которые читались реже. \n",
    "    - без применения весов idf лучший результат дало применение метода max. Это логично - ведь если возникнет ситуация когда пользователь прочитал две статьи, одна имеет по теме1 большой вес, другая по теме2, то использование усредняющих показателей в итоге усреднит значения тем для статей и по сути уберет полезную информацию о пользователе, тогда как параметр max создаст наиболее характерный вектор.\n",
    "    - при применении весов idf результаты работы модели ожидаемо возросли, при этом отметим, что лучший реультат досигнут при использовании усредняющих показателей - median или mean, смотря по какому параметру оценивать. Такой результат наблюдается в связи с тем, что теперь вектора статей имеют веса, и больше нельзя в столбце просто выбирать максимальный. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
